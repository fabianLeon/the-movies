{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stop_words = stopwords.words('spanish')\n",
    "stemmer = nltk.SnowballStemmer(\"spanish\")\n",
    "df = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding=\"latin-1\")\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_data(text):\n",
    "    # Clean puntuation, urls, and so on\n",
    "    text = clean_text(text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    # Stemm all the words in the sentence\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import (LSTM,\n",
    "                          Embedding,\n",
    "                          BatchNormalization,\n",
    "                          Dense,\n",
    "                          TimeDistributed,\n",
    "                          Dropout,\n",
    "                          Bidirectional,\n",
    "                          Flatten,\n",
    "                          GlobalMaxPool1D)\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from transformers import TFBertModel\n",
    "from transformers import BertTokenizer\n",
    "from collections import defaultdict\n",
    "# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df['target'])\n",
    "\n",
    "df['target_encoded'] = le.transform(df['target'])\n",
    "df.head()\n",
    "x = df['message_clean']\n",
    "y = df['target_encoded']\n",
    "\n",
    "print(len(x), len(y))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(x_train)\n",
    "x_train_dtm = vect.transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_transformer.fit(x_train_dtm)\n",
    "x_train_tfidf = tfidf_transformer.transform(x_train_dtm)\n",
    "\n",
    "x_train_tfidf\n",
    "texts = df['message_clean']\n",
    "target = df['target_encoded']\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(texts)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length\n",
    "sequences = [[1], [2, 3], [4, 5, 6]]\n",
    "tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=None, dtype='int32', padding='pre',\n",
    "    truncating='pre', value=0.0\n",
    ")\n",
    "\n",
    "\n",
    "def embed(corpus):\n",
    "    return word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "\n",
    "longest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "\n",
    "train_padded_sentences = pad_sequences(\n",
    "    embed(texts),\n",
    "    length_long_sentence,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "train_padded_sentences\n",
    "embeddings_dictionary = dict()\n",
    "embedding_dim = 100\n",
    "\n",
    "# Load GloVe 100D embeddings\n",
    "with open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt') as fp:\n",
    "    for line in fp.readlines():\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "embedding_matrix\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb.fit(x_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(x_test_dtm)\n",
    "y_pred_prob = nb.predict_proba(x_test_dtm)[:, 1]\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding=\"latin-1\")\n",
    "\n",
    "df = df.dropna(how=\"any\", axis=1)\n",
    "df.columns = ['target', 'message']\n",
    "\n",
    "df.head()\n",
    "df['message_len'] = df['message'].apply(lambda x: len(x.split(' ')))\n",
    "df.head()\n",
    "\n",
    "# Special thanks to https://www.kaggle.com/tanulsingh077 for this function\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df['message_clean'] = df['message'].apply(clean_text)\n",
    "df.head()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "more_stopwords = ['u', 'im', 'c']\n",
    "stop_words = stop_words + more_stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "\n",
    "df['message_clean'] = df['message_clean'].apply(remove_stopwords)\n",
    "df.head()\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemm_text(text):\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    return text\n",
    "\n",
    "\n",
    "df['message_clean'] = df['message_clean'].apply(stemm_text)\n",
    "df.head()\n",
    "\n",
    "\n",
    "def preprocess_data(text):\n",
    "    # Clean puntuation, urls, and so on\n",
    "    text = clean_text(text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    # Stemm all the words in the sentence\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "df['message_clean'] = df['message_clean'].apply(preprocess_data)\n",
    "df.head()\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df['target'])\n",
    "\n",
    "df['target_encoded'] = le.transform(df['target'])\n",
    "df.head()\n",
    "\n",
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "x = df['message_clean']\n",
    "y = df['target_encoded']\n",
    "\n",
    "print(len(x), len(y))\n",
    "# Split into train and test sets\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(x_train)\n",
    "CountVectorizer()\n",
    "# Use the trained to create a document-term matrix from train and test sets\n",
    "x_train_dtm = vect.transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "\n",
    "vect_tunned = CountVectorizer(stop_words='english', ngram_range=(\n",
    "    1, 2), min_df=0.1, max_df=0.7, max_features=100)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_transformer.fit(x_train_dtm)\n",
    "x_train_tfidf = tfidf_transformer.transform(x_train_dtm)\n",
    "\n",
    "x_train_tfidf\n",
    "\n",
    "texts = df['message_clean']\n",
    "target = df['target_encoded']\n",
    "\n",
    "\n",
    "# Calculate the length of our vocabulary\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(texts)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length\n",
    "\n",
    "tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=None, dtype='int32', padding='pre',\n",
    "    truncating='pre', value=0.0\n",
    ")\n",
    "\n",
    "\n",
    "def embed(corpus):\n",
    "    return word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "\n",
    "longest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "\n",
    "train_padded_sentences = pad_sequences(\n",
    "    embed(texts),\n",
    "    length_long_sentence,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "train_padded_sentences\n",
    "embeddings_dictionary = dict()\n",
    "embedding_dim = 100\n",
    "\n",
    "# Load GloVe 100D embeddings\n",
    "with open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt') as fp:\n",
    "    for line in fp.readlines():\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "# embeddings_dictionary\n",
    "# Now we will load embedding vectors of those words that appear in the\n",
    "# Glove dictionary. Others will be initialized to 0.\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "embedding_matrix\n",
    "\n",
    "\n",
    "# Create a Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb.fit(x_train_dtm, y_train)\n",
    "MultinomialNB()\n",
    "\n",
    "# Make class anf probability predictions\n",
    "y_pred_class = nb.predict(x_test_dtm)\n",
    "y_pred_prob = nb.predict_proba(x_test_dtm)[:, 1]\n",
    "# calculate accuracy of class predictions\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "# Calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "\n",
    "pipe = Pipeline([('bow', CountVectorizer()),\n",
    "                 ('tfid', TfidfTransformer()),\n",
    "                 ('model', MultinomialNB())])\n",
    "\n",
    "pipe.fit(x_train, y_train)\n",
    "\n",
    "y_pred_class = pipe.predict(x_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "    ('model', xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        max_depth=7,\n",
    "        n_estimators=80,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "        # colsample_bytree=0.8,\n",
    "        # subsample=0.7,\n",
    "        # min_child_weight=5,\n",
    "    ))\n",
    "])\n",
    "pipe.fit(x_train, y_train)\n",
    "\n",
    "y_pred_class = pipe.predict(x_test)\n",
    "y_pred_train = pipe.predict(x_train)\n",
    "\n",
    "print('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))\n",
    "print('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_padded_sentences,\n",
    "    target,\n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "\n",
    "def glove_lstm():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_matrix.shape[0],\n",
    "        output_dim=embedding_matrix.shape[1],\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=length_long_sentence\n",
    "    ))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        length_long_sentence,\n",
    "        return_sequences=True,\n",
    "        recurrent_dropout=0.2\n",
    "    )))\n",
    "\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(length_long_sentence, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(length_long_sentence, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = glove_lstm()\n",
    "model.summary()\n",
    "\n",
    "model = glove_lstm()\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    verbose=1,\n",
    "    patience=5,\n",
    "    min_lr=0.001\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=7,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[reduce_lr, checkpoint]\n",
    ")\n",
    "y_preds = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "\n",
    "def bert_encode(data, maximum_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in data:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=maximum_length,\n",
    "            pad_to_max_length=True,\n",
    "\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "\n",
    "texts = df['message_clean']\n",
    "\n",
    "\n",
    "target = df['target_encoded']\n",
    "\n",
    "train_input_ids, train_attention_masks = bert_encode(texts, 60)\n",
    "\n",
    "\n",
    "def create_model(bert_model):\n",
    "\n",
    "    input_ids = tf.keras.Input(shape=(60,), dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(60,), dtype='int32')\n",
    "\n",
    "    output = bert_model([input_ids, attention_masks])\n",
    "    output = output[1]\n",
    "    output = tf.keras.layers.Dense(32, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dropout(0.2)(output)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[input_ids, attention_masks], outputs=output)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = create_model(bert_model)\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    [train_input_ids, train_attention_masks],\n",
    "    target,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\",\n",
    "                 encoding=\"latin-1\")\n",
    "test_df = pd.read_csv(\n",
    "    \"/kaggle/input/nlp-getting-started/test.csv\", encoding=\"latin-1\")\n",
    "\n",
    "df = df.dropna(how=\"any\", axis=1)\n",
    "df['text_len'] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "df.head()\n",
    "\n",
    "balance_counts = df.groupby('target')['target'].agg('count').values\n",
    "balance_counts\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "more_stopwords = ['u', 'im', 'c']\n",
    "stop_words = stop_words + more_stopwords\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def preprocess_data(text):\n",
    "    # Clean puntuation, urls, and so on\n",
    "    text = clean_text(text)\n",
    "    # Remove stopwords and Stemm all the words in the sentence\n",
    "    text = ' '.join(stemmer.stem(word)\n",
    "                    for word in text.split(' ') if word not in stop_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "test_df['text_clean'] = test_df['text'].apply(preprocess_data)\n",
    "\n",
    "df['text_clean'] = df['text'].apply(preprocess_data)\n",
    "df.head()\n",
    "\n",
    "\n",
    "def create_corpus_df(tweet, target):\n",
    "    corpus = []\n",
    "\n",
    "    for x in tweet[tweet['target'] == target]['text_clean'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "corpus_disaster_tweets = create_corpus_df(df, 1)\n",
    "\n",
    "dic = defaultdict(int)\n",
    "for word in corpus_disaster_tweets:\n",
    "    dic[word] += 1\n",
    "\n",
    "top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "top\n",
    "corpus_disaster_tweets = create_corpus_df(df, 0)\n",
    "\n",
    "dic = defaultdict(int)\n",
    "for word in corpus_disaster_tweets:\n",
    "    dic[word] += 1\n",
    "\n",
    "top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "top\n",
    "# wc = WordCloud(\n",
    "#     background_color='white',\n",
    "#     max_words=200,\n",
    "#     mask=twitter_mask,\n",
    "# )\n",
    "#wc.generate(' '.join(text for text in df.loc[df['target'] == 0, 'text_clean']))\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.title('Top words for Fake messages',\n",
    "          fontdict={'size': 22,  'verticalalignment': 'bottom'})\n",
    "#plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "x = df['text_clean']\n",
    "y = df['target']\n",
    "\n",
    "# Split into train and test sets\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))\n",
    "pipe = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "    ('model', xgb.XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the pipeline with the data\n",
    "pipe.fit(x_train, y_train)\n",
    "\n",
    "y_pred_class = pipe.predict(x_test)\n",
    "y_pred_train = pipe.predict(x_train)\n",
    "\n",
    "print('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))\n",
    "print('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "\n",
    "#conf_matrix(metrics.confusion_matrix(y_test, y_pred_class))\n",
    "train_tweets = df['text_clean'].values\n",
    "test_tweets = test_df['text_clean'].values\n",
    "train_target = df['target'].values\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(train_tweets)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length\n",
    "longest_train = max(\n",
    "    train_tweets, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "\n",
    "train_padded_sentences = pad_sequences(\n",
    "    embed(train_tweets),\n",
    "    length_long_sentence,\n",
    "    padding='post'\n",
    ")\n",
    "test_padded_sentences = pad_sequences(\n",
    "    embed(test_tweets),\n",
    "    length_long_sentence,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "train_padded_sentences\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "embedding_matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_padded_sentences,\n",
    "    train_target,\n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "\n",
    "model = glove_lstm()\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    verbose=1,\n",
    "    patience=5,\n",
    "    min_lr=0.001\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=7,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[reduce_lr, checkpoint]\n",
    ")\n",
    "#plot_learning_curves(history, [['loss', 'val_loss'], [\n",
    "#                     'accuracy', 'val_accuracy']])\n",
    "preds = model.predict_classes(X_test)\n",
    "#show_metrics(preds, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
